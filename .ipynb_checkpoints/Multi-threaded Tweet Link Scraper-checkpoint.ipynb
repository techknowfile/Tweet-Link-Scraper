{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed: 2625, Failed: 1055, Time elapsed: 4:09:22.131421, items/second:  0 \n",
      "None\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re, json\n",
    "from IPython.display import clear_output\n",
    "import threading\n",
    "import sys\n",
    "import queue\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# CONFIG\n",
    "JSON_FILE_LOCATION = 'tweet_data_original.json'\n",
    "NUM_OF_THREADS = 20\n",
    "\n",
    "# EVIL GLOBALS\n",
    "status = (0, 0, None, None, None)\n",
    "EOF = 'EOF'\n",
    "f = open(JSON_FILE_LOCATION, encoding='utf8')\n",
    "lock = threading.Lock()\n",
    "input_lock = threading.Lock()\n",
    "q = queue.Queue()\n",
    "passed = 0\n",
    "failed = 0\n",
    "\n",
    "def chunkify(lst,n):\n",
    "    return [lst[i::n] for i in range(n)]\n",
    "\n",
    "def get_tweet():\n",
    "    global f\n",
    "    with input_lock:\n",
    "        try:\n",
    "            while(True):\n",
    "                line = f.readline()\n",
    "                if line == '':\n",
    "                    return EOF\n",
    "                tweet = json.loads(line)\n",
    "                if len(tweet['entities']['urls']) > 0:\n",
    "                    #print(tweet['entities']['urls'][0]['url'])\n",
    "                    return tweet\n",
    "        except:\n",
    "            return EOF\n",
    "    \n",
    "def visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element)) or re.match('<![endif].*', str(element)):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "    \n",
    "def main():\n",
    "    threads = []\n",
    "    qt = threading.Thread(target=queue_worker)\n",
    "    qt.daemon = True\n",
    "    qt.start()\n",
    "    \n",
    "    for i in range(NUM_OF_THREADS):\n",
    "        t = threading.Thread(target=tweet_link_scraper_worker)\n",
    "        t.daemon = True\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "        \n",
    "    while True:\n",
    "        finished = True\n",
    "        for thread in threads:\n",
    "            if thread.isAlive():\n",
    "                finished = False\n",
    "                \n",
    "        clear_output(wait=True)\n",
    "        print(\"Passed: {}, Failed: {}, Time elapsed: {}, items/second:  {} \\n{}\".format(status[0], status[1], status[2], status[3], status[4]))\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(2)\n",
    "        if finished:\n",
    "            break\n",
    "    \n",
    "    q.join()\n",
    "    print(\"Done.\")\n",
    "    f.close()\n",
    "    \n",
    "def return_worker():\n",
    "    global status\n",
    "    start_time = datetime.datetime.now()\n",
    "    passed = 0\n",
    "    failed = 0\n",
    "    while True:\n",
    "        item = q.get()\n",
    "        error = do_queue_work(item)\n",
    "        if error == 0:\n",
    "            passed += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "        time_delta = datetime.datetime.now() - start_time\n",
    "        seconds_elapsed = time_delta.total_seconds() if time_delta.total_seconds() > 0 else 1\n",
    "        average_item_per_second = int((passed + failed)/seconds_elapsed)\n",
    "        #clear_output()\n",
    "        status = (passed, failed, time_delta, average_item_per_second, item)\n",
    "        q.task_done()\n",
    "        \n",
    "def do_queue_work(item):\n",
    "    if item == None:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def tweet_link_scraper_worker():\n",
    "    global passed, failed, total\n",
    "    while(True):\n",
    "        tweet = get_tweet()\n",
    "        data = None\n",
    "        if tweet == EOF:\n",
    "            break\n",
    "        else:\n",
    "            try:\n",
    "                with urllib.request.urlopen(tweet['entities']['urls'][0]['url']) as response:\n",
    "                    html = response.read()\n",
    "                    url = response.geturl()\n",
    "                    soup = bs(html, 'html.parser')\n",
    "                    text = soup.findAll(text=True)\n",
    "                    text_items = filter(visible, text)\n",
    "                    visible_text = [item.strip() for item in text_items if item != '\\n' and len(item.strip().split(' ')) > 10]\n",
    "                    data = visible_text\n",
    "            except:\n",
    "                # Could not access address\n",
    "                # print(tweet['entities']['urls'][0]['url'])\n",
    "                pass\n",
    "\n",
    "            finally:\n",
    "                q.put(data)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
