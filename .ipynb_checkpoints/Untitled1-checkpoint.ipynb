{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed: 0, Failed: 0, Time elapsed: None, items/second:  None \n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re, json\n",
    "from IPython.display import clear_output\n",
    "import threading\n",
    "import sys\n",
    "import queue\n",
    "import datetime\n",
    "import time\n",
    "from multiprocessing import Process, Queue, Lock\n",
    "\n",
    "# CONFIG\n",
    "JSON_FILE_LOCATION = 'tweet_data_original.json'\n",
    "NUM_OF_THREADS = 20\n",
    "NUM_OF_PROCESSES = 4\n",
    "\n",
    "# EVIL GLOBALS\n",
    "status = (0, 0, None, None)\n",
    "poison_pill = 'EOF'\n",
    "EOF = 'EOF'\n",
    "f = open(JSON_FILE_LOCATION, encoding='utf8')\n",
    "lock = Lock()\n",
    "input_lock = Lock()\n",
    "passed = 0\n",
    "failed = 0\n",
    "\n",
    "def chunkify(lst,n):\n",
    "    return [lst[i::n] for i in range(n)]\n",
    "\n",
    "def get_tweet():\n",
    "    global f\n",
    "    with input_lock:\n",
    "        try:\n",
    "            while(True):\n",
    "                line = f.readline()\n",
    "                if line == '':\n",
    "                    return EOF\n",
    "                tweet = json.loads(line)\n",
    "                if len(tweet['entities']['urls']) > 0:\n",
    "                    #print(tweet['entities']['urls'][0]['url'])\n",
    "                    return tweet\n",
    "        except:\n",
    "            return EOF\n",
    "    \n",
    "def visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element)) or re.match('<![endif].*', str(element)):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def proc_worker(q, r):\n",
    "    print('proc started')\n",
    "    threads = []\n",
    "    \n",
    "    for i in range(NUM_OF_THREADS):\n",
    "        t = threading.Thread(target=thread_worker, args=(q,r,))\n",
    "        t.daemon = True\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "        \n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    \n",
    "def main():   \n",
    "    # Queue for cross Process/Thread communication\n",
    "    q = Queue() # tweet queue\n",
    "    r = Queue() # return queue\n",
    "    \n",
    "    t = threading.Thread(target=fill_queue_async, args=(q,))\n",
    "    t.daemon = True\n",
    "    t.start()\n",
    "  \n",
    "    # Thread for processing return values and status output\n",
    "    rt = threading.Thread(target=return_worker, args=(r,))\n",
    "    rt.daemon = True\n",
    "    rt.start()\n",
    "\n",
    "    processes = []\n",
    "    for i in range(NUM_OF_PROCESSES):\n",
    "        p = Process(target=proc_worker, args=(q,r,))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "        \n",
    "    t.join()\n",
    "    \n",
    "    # do stat printing stuff\n",
    "    while True:\n",
    "        finished = True\n",
    "        for p in processes:\n",
    "            if p.is_alive():\n",
    "                finished = False\n",
    "                \n",
    "        clear_output(wait=True)\n",
    "        print(\"Passed: {}, Failed: {}, Time elapsed: {}, items/second:  {} \\n\".format(status[0], status[1], status[2], status[3]))\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(2)\n",
    "        if finished:\n",
    "            break\n",
    "    \n",
    "    print(\"Done.\")\n",
    "    \n",
    "def fill_queue_async(q):\n",
    "    with open(JSON_FILE_LOCATION, encoding='utf8') as f:\n",
    "        while True:\n",
    "            if q.qsize() < 10000:\n",
    "                line = f.readline()\n",
    "                if not line:\n",
    "                    q.put(poison_pill)\n",
    "                    break\n",
    "                tweet = json.loads(line)\n",
    "                if len(tweet['entities']['urls']) > 0:\n",
    "                    #print(tweet['entities']['urls'][0]['url'])\n",
    "                    q.put(tweet)\n",
    "                \n",
    "            else:\n",
    "                print('Queue full.')\n",
    "\n",
    "def return_worker(r):\n",
    "    global status\n",
    "    start_time = datetime.datetime.now()\n",
    "    passed = 0\n",
    "    failed = 0\n",
    "    while True:\n",
    "        item = r.get()\n",
    "        error = do_queue_work(item)\n",
    "        if error == 0:\n",
    "            passed += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "        time_delta = datetime.datetime.now() - start_time\n",
    "        seconds_elapsed = time_delta.total_seconds() if time_delta.total_seconds() > 0 else 1\n",
    "        average_item_per_second = int((passed + failed)/seconds_elapsed)\n",
    "        #clear_output()\n",
    "        status = (passed, failed, time_delta, average_item_per_second)\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "def do_queue_work(item):\n",
    "    if item == None:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def thread_worker(q, r):\n",
    "    global passed, failed, total\n",
    "    while(True):\n",
    "        doc = {}\n",
    "        tweet = q.get()\n",
    "        data = None\n",
    "        if tweet == poison_pill:\n",
    "            q.put(poison_pill)\n",
    "            break\n",
    "        else:\n",
    "            try:\n",
    "                with urllib.request.urlopen(tweet['entities']['urls'][0]['url']) as response:\n",
    "                    html = response.read()\n",
    "                    url = response.geturl()\n",
    "                    soup = bs(html, 'html.parser')\n",
    "                    text = soup.findAll(text=True)\n",
    "                    text_items = filter(visible, text)\n",
    "                    visible_text = [item.strip() for item in text_items if item != '\\n' and len(item.strip().split(' ')) > 10]\n",
    "                    doc['texts'] = visible_text\n",
    "                    doc['images'] = [tag['src'] for tag in bs.findAll(itemprop='image')]\n",
    "                    doc['url'] = url\n",
    "            except:\n",
    "                #Could not access address\n",
    "                #print(tweet['entities']['urls'][0]['url'])\n",
    "                pass\n",
    "\n",
    "            finally:\n",
    "                r.put(doc)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
